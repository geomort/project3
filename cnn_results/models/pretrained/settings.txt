learning_rate = 0.001

if PRETRAINED_MODEL:
    optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=1e-4)  # only updates weights for fully connected layer
else:
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)

loss_fn = nn.CrossEntropyLoss()
epochs = 50 #2#50#30



Training dataset
Per-class counts:
 0            ants: 1029
 1            bees: 1101
 2         beetles: 857
 3    caterpillars: 906
 4      earthworms: 717
 5         earwigs: 939
 6    grasshoppers: 1044
 7           moths: 1059
 8           slugs: 797
 9          snails: 1079
10           wasps: 1011
11         weevils: 960

Validation dataset
Per-class counts:
 0            ants: 95
 1            bees: 99
 2         beetles: 89
 3    caterpillars: 77
 4      earthworms: 51
 5         earwigs: 91
 6    grasshoppers: 98
 7           moths: 100
 8           slugs: 77
 9          snails: 98
10           wasps: 116
11         weevils: 104

Test dataset
Per-class counts:
 0            ants: 54
 1            bees: 40
 2         beetles: 41
 3    caterpillars: 46
 4      earthworms: 27
 5         earwigs: 59
 6    grasshoppers: 38
 7           moths: 47
 8           slugs: 46
 9          snails: 44
10           wasps: 46
11         weevils: 58





# Settings
yaml_path = Path.cwd().parent / "datasets" / "agropest12" / "data.yaml"

# Settings for retreiving images. Set None for all images or an integer for subset for testing code
subset_classes = None #['Ants', 'Snails'] # None or list of classes. Example ['Ants', 'Snails']
n_images_train  = None #None for all images
n_images_valid = None # int(n_images_train * 0.2) #20% of number of images for train #None for all images
n_images_test = None #int(n_images_train * 0.2) #20% of number of images for train #None for all images


def get_batch_size(n_images_train=None):
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3  # in GB
        if gpu_mem > 12:
            default_large = 128
        else:
            default_large = 64
    else:
        default_large = 32

    if n_images_train is None:
        return default_large
    elif n_images_train < 20:
        return 16
    elif n_images_train < 100:
        return 32
    else:
        return default_large

batch_size = get_batch_size(n_images_train)


PRETRAINED_MODEL = True   # Use pretrained model and weights, else custom
if PRETRAINED_MODEL:
    NORMALIZE = 'ImageNet' #'auto' -  load if file exists, else compute and save, 'compute' - always recompute and overwrite file., 'load' - load from file. 'ImageNet' - use ImageNet values
else:
    NORMALIZE = 'load'
NORMALIZE_CACHE_PATH =  Path.cwd().parent / "datasets" / "agropest12" / "cache" / "agropest12_norm.json"

IMG_SIZE= (224,224) # must use (224,224) to match imagenet            #(128, 128) # # # # Image size for CNN (transform)
CROP_MODE = 'largest'# 'largest' #'largest'  # 'none' or 'largest  # largest extracts part of image where largest object is located. Some images have mostly background and small portion of object

DEBUG = False # Print debug info for functions
QUALITY_CONTROL = True # print and plot information for quality control
SAVE_MODELS = True
SHOW_PLOT = True
SAVE_FIGURE = True
COMPUTE_AVERAGE_IMG_SIZE = False


# Paths fir saving figures and models
current_dir = Path.cwd()
parent_dir = current_dir.parent

# Directory for saving figures
figures_path = parent_dir / "figures"
figures_path.mkdir(parents=True, exist_ok=True)

# Directory for saving models and summaries
models_path = parent_dir / "models"
models_path.mkdir(parents=True, exist_ok=True)


# Current timestamp for files
timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M")



