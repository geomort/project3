{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb84d0c8",
   "metadata": {},
   "source": [
    "# Question to Copilot about how to do hyperparameter search in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6cdcc5",
   "metadata": {},
   "source": [
    "## Question 1: How to do dynamic hyperparameter test in PyTorch?\n",
    "\n",
    "Answers below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af629796",
   "metadata": {},
   "source": [
    "### 1. Use Learning Rate Schedulers\n",
    "PyTorch provides built-in schedulers in torch.optim.lr_scheduler to dynamically change the learning rate during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Example: Reduce LR on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = torch.randn(1)  # Dummy loss\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3971ce",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter Sweeps with Libraries\n",
    "For dynamic testing across multiple runs, use tools like:\n",
    "\n",
    "Ray Tune\n",
    "Optuna\n",
    "Weights & Biases Sweeps\n",
    "\n",
    "These allow you to:\n",
    "\n",
    "Define a search space for hyperparameters.\n",
    "Dynamically sample and adjust based on performance.\n",
    "\n",
    "Example with Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    # Train your PyTorch model here using lr and batch_size\n",
    "    return validation_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3fb86",
   "metadata": {},
   "source": [
    "### 3. Dynamic Adjustment During Training\n",
    "You can implement custom logic to adjust hyperparameters based on metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(...)\n",
    "    \n",
    "    # Example: Adjust learning rate dynamically\n",
    "    if train_loss < threshold:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c639ea",
   "metadata": {},
   "source": [
    "### 4. Combine with Callbacks\n",
    "If you use PyTorch Lightning, you can add callbacks for dynamic hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = pl.Trainer(callbacks=[lr_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca951c",
   "metadata": {},
   "source": [
    "## Question 2: I already use: Example: Reduce LR on plateau scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10) should i use optune as well?\n",
    "\n",
    "Answers below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adea7d",
   "metadata": {},
   "source": [
    "Great question! If youâ€™re already using ReduceLROnPlateau, that handles dynamic learning rate adjustment during training, which is good for convergence.\n",
    "Optuna (or similar tools) serves a different purpose:\n",
    "\n",
    "It optimizes hyperparameters across multiple training runs, not just the learning rate but also things like:\n",
    "\n",
    "Initial learning rate\n",
    "Batch size\n",
    "Weight decay\n",
    "Dropout rate\n",
    "Model architecture parameters (e.g., number of layers, hidden units)\n",
    "\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "ReduceLROnPlateau = dynamic adjustment within a single run\n",
    "Optuna = systematic search across runs\n",
    "\n",
    "If your goal is finding the best overall hyperparameter set, Optuna is worth adding. If youâ€™re only fine-tuning LR during training, then your current setup is enough.\n",
    "âœ… Best practice: Use Optuna to find a good starting point for hyperparameters, then keep ReduceLROnPlateau for fine-grained control during training.\n",
    "\n",
    "Do you want me to:\n",
    "\n",
    "Show you a full Optuna integration example with PyTorch, including ReduceLROnPlateau inside the trial?\n",
    "Or give you a comparison of Optuna vs Ray Tune vs manual grid search for your case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04707601",
   "metadata": {},
   "source": [
    "## Question 3: Full Optuna integration example with PyTorch\n",
    "\n",
    "Answers below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba80b4",
   "metadata": {},
   "source": [
    "Hereâ€™s a full Optuna integration example with PyTorch, including ReduceLROnPlateau inside the trial loop:\n",
    "\n",
    "âœ… Complete Example: Optuna + PyTorch + ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ddfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "\n",
    "# Dummy dataset\n",
    "X = torch.randn(1000, 10)\n",
    "y = torch.randn(1000, 1)\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "\n",
    "    # DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = SimpleModel(hidden_size, dropout)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_X)\n",
    "            loss = criterion(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    # Validation metric (here using training loss as proxy)\n",
    "    return avg_loss\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1be02b",
   "metadata": {},
   "source": [
    "What this does\n",
    "\n",
    "Optuna searches over:\n",
    "\n",
    "hidden_size (model architecture)\n",
    "dropout\n",
    "lr (initial learning rate)\n",
    "batch_size\n",
    "weight_decay\n",
    "\n",
    "\n",
    "Inside each trial:\n",
    "\n",
    "We train for 20 epochs.\n",
    "Use ReduceLROnPlateau to dynamically adjust LR during training.\n",
    "\n",
    "\n",
    "After trials, Optuna gives the best hyperparameter set.\n",
    "\n",
    "\n",
    "Next Steps\n",
    "\n",
    "Replace the dummy dataset with your real dataset.\n",
    "Add a proper validation split and use validation loss for scheduler.step() and Optunaâ€™s return value.\n",
    "Optionally integrate early stopping and pruning with Optuna for faster optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fad703",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.report(avg_loss, epoch)\n",
    "if trial.should_prune():\n",
    "    raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280283d",
   "metadata": {},
   "source": [
    "Do you want me to extend this example with Optuna pruning and PyTorch Lightning integration for a more production-ready setup? Or keep it vanilla PyTorch but add validation and early stopping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a04573",
   "metadata": {},
   "source": [
    "## Question 4: I do train as\n",
    "\n",
    "Answers below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a916d2",
   "metadata": {},
   "source": [
    "\n",
    "Parameters for - train model\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_flexible.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "Train with early stopping + scheduler\n",
    "history = train_model(\n",
    "    model=model_flexible,\n",
    "    num_epochs=epochs,\n",
    "    train_dl=train_dataloader,\n",
    "    valid_dl=validation_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device='cpu',\n",
    "    verbose=True,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "and set model with\n",
    "model_flexible = FlexibleCNN(\n",
    "    input_size=input_size_img,\n",
    "    num_classes=num_classes,\n",
    "    conv_layers=[(32, 3), (64, 3), (128, 3)],\n",
    "    fc_layers=[256, 128],\n",
    "    activation=nn.ReLU,    # Custom activation\n",
    "    dropout_fc=0.3,             # Dropout in FC layers\n",
    "    dropout_conv=0.1,           # Dropout in conv layers\n",
    "    use_batchnorm=True,\n",
    "    pool_type=\"avg\",            # Use AvgPool instead of MaxPool\n",
    "    global_pool=\"max\"           # Use AdaptiveMaxPool for final pooling\n",
    ")\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes,\n",
    "                 conv_layers=[(32, 3), (64, 3), (128, 3)],  # [(out_channels, kernel_size), ...]\n",
    "                 fc_layers=[256, 128],                     # [units for each FC layer]\n",
    "                 activation=nn.ReLU,                       # Activation class\n",
    "                 dropout_fc=0.5,                           # Dropout for FC layers\n",
    "                 dropout_conv=0.0,                         #  Optional dropout for conv layers\n",
    "                 use_batchnorm=True,\n",
    "                 pool_type=\"max\",                          # \"max\" or \"avg\"\n",
    "                 global_pool=\"avg\",                        #  \"avg\" or \"max\" for final pooling\n",
    "                 show_summary=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Choose pooling layers\n",
    "        pool_layer = nn.MaxPool2d(2) if pool_type == \"max\" else nn.AvgPool2d(2)\n",
    "        global_pool_layer = nn.AdaptiveAvgPool2d((1, 1)) if global_pool == \"avg\" else nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "        # Build convolutional feature extractor\n",
    "        layers = []\n",
    "        in_channels = input_size[0]\n",
    "        for out_channels, kernel_size in conv_layers:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(activation())\n",
    "            if dropout_conv > 0:\n",
    "                layers.append(nn.Dropout2d(dropout_conv))  # âœ… Dropout for conv layers\n",
    "            layers.append(pool_layer)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        layers.append(global_pool_layer)  # âœ… Global pooling\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Classifier\n",
    "        in_features = conv_layers[-1][0]\n",
    "        fc = []\n",
    "        for units in fc_layers:\n",
    "            fc.append(nn.Linear(in_features, units))\n",
    "            fc.append(activation())\n",
    "            if dropout_fc > 0:\n",
    "                fc.append(nn.Dropout(dropout_fc))\n",
    "            in_features = units\n",
    "        fc.append(nn.Linear(in_features, num_classes))\n",
    "        self.classifier = nn.Sequential(*fc)\n",
    "\n",
    "        # Optional summary\n",
    "        if show_summary:\n",
    "            summary(self, input_size=(1, *input_size), col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bd62c",
   "metadata": {},
   "source": [
    "Thanks for sharing your setup! You already have a custom CNN architecture, early stopping, and a scheduler in your train_model function. To integrate Optuna with this workflow, weâ€™ll wrap your training logic inside an Optuna objective function and make the hyperparameters dynamic.\n",
    "Hereâ€™s how you can do it:\n",
    "\n",
    "âœ… Optuna Integration with Your FlexibleCNN and train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    dropout_fc = trial.suggest_float('dropout_fc', 0.1, 0.5)\n",
    "    dropout_conv = trial.suggest_float('dropout_conv', 0.0, 0.3)\n",
    "    activation_choice = trial.suggest_categorical('activation', [nn.ReLU, nn.LeakyReLU])\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    conv_layers = [(trial.suggest_int('conv1', 16, 64), 3),\n",
    "                   (trial.suggest_int('conv2', 32, 128), 3),\n",
    "                   (trial.suggest_int('conv3', 64, 256), 3)]\n",
    "    fc_layers = [trial.suggest_int('fc1', 128, 512),\n",
    "                 trial.suggest_int('fc2', 64, 256)]\n",
    "\n",
    "    # Create model with dynamic architecture\n",
    "    model_flexible = FlexibleCNN(\n",
    "        input_size=input_size_img,\n",
    "        num_classes=num_classes,\n",
    "        conv_layers=conv_layers,\n",
    "        fc_layers=fc_layers,\n",
    "        activation=activation_choice,\n",
    "        dropout_fc=dropout_fc,\n",
    "        dropout_conv=dropout_conv,\n",
    "        use_batchnorm=True,\n",
    "        pool_type=\"avg\",\n",
    "        global_pool=\"max\",\n",
    "        show_summary=False\n",
    "    )\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.Adam(model_flexible.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # DataLoader with dynamic batch size\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Train with early stopping and scheduler\n",
    "    history = train_model(\n",
    "        model=model_flexible,\n",
    "        num_epochs=30,\n",
    "        train_dl=train_dl,\n",
    "        valid_dl=valid_dl,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device='cpu',\n",
    "        verbose=False,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # Use best validation loss as metric\n",
    "    best_val_loss = min(history['val_loss'])\n",
    "    trial.report(best_val_loss, step=0)\n",
    "\n",
    "    # Enable pruning\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7253067",
   "metadata": {},
   "source": [
    "Key Features\n",
    "\n",
    "Dynamic architecture tuning: Conv layer sizes, FC layer sizes, dropout, activation.\n",
    "Dynamic training hyperparameters: Learning rate, batch size.\n",
    "Early stopping + scheduler: Still used inside train_model.\n",
    "Optuna pruning: Stops bad trials early for efficiency.\n",
    "\n",
    "\n",
    "âœ… Next Steps\n",
    "\n",
    "Replace train_dataset and valid_dataset with your actual datasets.\n",
    "Add GPU support (device='cuda' if available).\n",
    "Log results with Optuna dashboard (optuna-dashboard).\n",
    "\n",
    "\n",
    "ðŸ‘‰ Do you want me to add Optunaâ€™s pruning callbacks for PyTorch Lightning, or keep it vanilla PyTorch but include visualization of the search results (e.g., hyperparameter importance, optimization history)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4155 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
