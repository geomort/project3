{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7112f321",
   "metadata": {},
   "source": [
    "# Classification of images using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dc0b0",
   "metadata": {},
   "source": [
    "## Reduction of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "\n",
    "# Custom import\n",
    "from source.dataset_reduction import extract_reduced_dataset, check_images_size_equal\n",
    "from source.cnn_model import train_model, FlexibleCNN\n",
    "from source.plotting import plot_training_history, plot_random_predictions, plot_filter_weights, plot_feature_maps, plot_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132a4ba",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "\n",
    "hyperparameter\n",
    "- used optuna, needs to be explained\n",
    "\n",
    "\n",
    "comment code\n",
    "- done for plotting and this jupyter\n",
    "\n",
    "\n",
    "Test vs pre-built model\n",
    "- use pre-built model as paper to read?\n",
    "\n",
    "architechture\n",
    "- how to choose? add pooling and type to model generator\n",
    "\n",
    "\n",
    "search \"ML nodes UiO\"\n",
    "- apply for access\n",
    "- \n",
    "\n",
    "\n",
    "data augmentation? already augmented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a90ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for saving figures, models, etc\n",
    "\n",
    "SAVE_FIGURES = True\n",
    "SHOW_PLOT = True\n",
    "\n",
    "SAVE_MODELS = True\n",
    "\n",
    "# Optuna used for hyperparameter search\n",
    "USE_OPTUNA = True\n",
    "OPTUNA_N_TRIALS = 100\n",
    "\n",
    "\n",
    "# Values for - Extract a reduced dataset for quick experiments\n",
    "images_for_train_validate_test = 20   # 546 is all, extracts  smaller subset if given\n",
    "# Extract only these classes\n",
    "classes_reduced = ['snail', 'wasp', 'moth']\n",
    "\n",
    "\n",
    "\n",
    "# Paths\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "# Directory for saving figures\n",
    "figures_path = parent_dir / \"figures\"\n",
    "figures_path.mkdir(parents=True, exist_ok=True)\n",
    "# Directory for saving models and summaries\n",
    "models_path = parent_dir / \"models\"\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Current timestamp for files\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a reduced dataset for quick experiments\n",
    "dataset = extract_reduced_dataset(images_for_train_validate_test, classes_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c20b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that images have equal size\n",
    "check_images_size_equal(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7ab6d",
   "metadata": {},
   "source": [
    "## Prepare PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b73395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),  # 256,256 / 128,128 / 64,64\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=dataset / 'train/images', transform=transform) # resize images if needed\n",
    "test_dataset = datasets.ImageFolder(root=dataset / 'test/images', transform=transform) # resize images if needed\n",
    "validation_dataset = datasets.ImageFolder(root=dataset / 'valid/images', transform=transform) # resize images if needed\n",
    "\n",
    "\n",
    "if len(train_dataset) < 20:\n",
    "    batch_size = 16\n",
    "elif len(train_dataset) > 20 and len(train_dataset) < 100:\n",
    "    batch_size = 32\n",
    "else:\n",
    "    batch_size = 64 \n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality control of DataLoader\n",
    "dataloader_verbose = True\n",
    "if dataloader_verbose:\n",
    "    ## Get one batch from DataLoader\n",
    "    images, labels = next(iter(train_dataloader))\n",
    "\n",
    "    print(f\"Shape: {images.shape}\")\n",
    "    print(f'Scaled data in in range [0,1], values in loaded image are: [{images.min().item()}, {images.max().item()}]')\n",
    "    print(f'Classes are: {train_dataset.class_to_idx}')\n",
    "    #print(f'Targets are: {train_dataset.targets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfcd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining CNN model in PyTorch using nn.Module\n",
    "\n",
    "input_size_img = tuple(images.shape[1:])  # (channels, height, width)\n",
    "num_classes = torch.unique(torch.tensor(train_dataset.targets)).numel()\n",
    "\n",
    "print(f'Images channel and size: {input_size_img}\\nNumber of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5101515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible CNN\n",
    "\n",
    "\"\"\"\n",
    "model_flexible = FlexibleCNN(\n",
    "    input_size=input_size_img,\n",
    "    num_classes=num_classes,\n",
    "    conv_layers=[(32, 3), (64, 3), (128, 3)],\n",
    "    fc_layers=[256, 128],\n",
    "    activation=nn.ReLU,    # Custom activation\n",
    "    dropout_fc=0.3,             # Dropout in FC layers\n",
    "    dropout_conv=0.1,           # Dropout in conv layers\n",
    "    use_batchnorm=True,\n",
    "    pool_type=\"avg\",            # Use AvgPool instead of MaxPool\n",
    "    global_pool=\"max\"           # Use AdaptiveMaxPool for final pooling\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62175292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OPTUNA:\n",
    "\n",
    "    def objective(trial):\n",
    "        # --- Hyperparameter suggestions ---\n",
    "        \"\"\"\n",
    "        # Sample vaues within range\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        dropout_fc = trial.suggest_float('dropout_fc', 0.1, 0.5)\n",
    "        dropout_conv = trial.suggest_float('dropout_conv', 0.0, 0.3)\n",
    "        activation_choice = trial.suggest_categorical('activation', [nn.ReLU, nn.LeakyReLU])\n",
    "        batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "        conv_layers = [\n",
    "            (trial.suggest_int('conv1', 16, 64), 3),\n",
    "            (trial.suggest_int('conv2', 32, 128), 3),\n",
    "            (trial.suggest_int('conv3', 64, 256), 3)\n",
    "        ]\n",
    "        fc_layers = [\n",
    "            trial.suggest_int('fc1', 128, 512),\n",
    "            trial.suggest_int('fc2', 64, 256)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # Use suggested categorical values\n",
    "        learning_rate = trial.suggest_categorical('learning_rate', [0.0001, 0.001, 0.01, 0.1])\n",
    "        dropout_fc = trial.suggest_categorical('dropout_fc', [0.1, 0.5])\n",
    "        dropout_conv = trial.suggest_categorical('dropout_conv', [0.0, 0.3])\n",
    "        activation_name = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU'])\n",
    "        activation_choice = nn.ReLU if activation_name == 'ReLU' else nn.LeakyReLU\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "        conv_layers = [\n",
    "            (trial.suggest_categorical('conv1', [16, 32, 64]), 3),\n",
    "            (trial.suggest_categorical('conv2', [32, 64, 128]), 3),\n",
    "            (trial.suggest_categorical('conv3', [64, 128, 256]), 3)\n",
    "        ]\n",
    "        fc_layers = [\n",
    "            trial.suggest_categorical('fc1', [128, 256, 512]),\n",
    "            trial.suggest_categorical('fc2', [64, 128, 256])\n",
    "        ]\n",
    "\n",
    "        # --- Model ---\n",
    "        model = FlexibleCNN(\n",
    "            input_size=input_size_img,\n",
    "            num_classes=num_classes,\n",
    "            conv_layers=conv_layers,\n",
    "            fc_layers=fc_layers,\n",
    "            activation=activation_choice,\n",
    "            dropout_fc=dropout_fc,\n",
    "            dropout_conv=dropout_conv,\n",
    "            use_batchnorm=True,\n",
    "            pool_type=\"avg\",\n",
    "            global_pool=\"max\",\n",
    "            show_summary=False\n",
    "        )\n",
    "\n",
    "        # --- Optimizer & Loss ---\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # --- DataLoader ---\n",
    "        train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_dl = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "\n",
    "        # --- Training loop with pruning ---\n",
    "        num_epochs = 30\n",
    "        patience = 5\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Train one epoch\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for X_batch, y_batch in train_dl:\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(X_batch)\n",
    "                loss = loss_fn(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_dl)\n",
    "\n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in valid_dl:\n",
    "                    preds = model(X_val)\n",
    "                    loss = loss_fn(preds, y_val)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(valid_dl)\n",
    "\n",
    "            # Report intermediate result to Optuna\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            # Check pruning\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    break\n",
    "\n",
    "        return best_val_loss\n",
    "\n",
    "    # --- Run Optuna study ---\n",
    "    \n",
    "    db_path = models_path / \"optuna_study.db\"\n",
    "    optuna_path = f\"sqlite:///{db_path}\"  # Convert to string for Optuna\n",
    "\n",
    "    study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(), \n",
    "        #storage='sqlite:///optuna_study.db',  # Save trials to SQLite\n",
    "        storage=optuna_path,  # Save trials to SQLite\n",
    "        load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=OPTUNA_N_TRIALS)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e71ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and save Optuna figures\n",
    "if USE_OPTUNA:\n",
    "    from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "    # Generate plots\n",
    "    fig_history = plot_optimization_history(study)\n",
    "    fig_importance = plot_param_importances(study)\n",
    "\n",
    "    # Show in notebook\n",
    "    if SHOW_PLOT:\n",
    "        fig_history.show()\n",
    "        fig_importance.show()\n",
    "\n",
    "    if SAVE_FIGURES:\n",
    "        path_history = Path(figures_path / f'{timestamp}_optuna_history_number_images{images_for_train_validate_test}.png')\n",
    "        path_importance = Path(figures_path / f'{timestamp}_optuna_importance_number_images{images_for_train_validate_test}.png')\n",
    "\n",
    "        # Save as PNG\n",
    "        fig_history.write_image(path_history)\n",
    "        fig_importance.write_image(path_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with Optuna or manual\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    # Update model with best hyperparameters from Optuna\n",
    "    best_params = study.best_params\n",
    "\n",
    "    model_flexible = FlexibleCNN(\n",
    "        input_size=input_size_img,\n",
    "        num_classes=num_classes,\n",
    "        conv_layers=[\n",
    "            (best_params['conv1'], 3),\n",
    "            (best_params['conv2'], 3),\n",
    "            (best_params['conv3'], 3)\n",
    "        ],\n",
    "        fc_layers=[\n",
    "            best_params['fc1'],\n",
    "            best_params['fc2']\n",
    "        ],\n",
    "        activation=best_params['activation'],   \n",
    "        dropout_fc=best_params['dropout_fc'],\n",
    "        dropout_conv=best_params['dropout_conv'],\n",
    "        use_batchnorm=True,\n",
    "        pool_type=\"avg\",\n",
    "        global_pool=\"max\"\n",
    "    )\n",
    "else: # choosing settings manually\n",
    "    model_flexible = FlexibleCNN(\n",
    "    input_size=input_size_img,\n",
    "    num_classes=num_classes,\n",
    "    conv_layers=[(32, 3), (64, 3), (128, 3)],\n",
    "    fc_layers=[256, 128],\n",
    "    activation=nn.ReLU,    # Custom activation\n",
    "    dropout_fc=0.3,             # Dropout in FC layers\n",
    "    dropout_conv=0.1,           # Dropout in conv layers\n",
    "    use_batchnorm=True,\n",
    "    pool_type=\"avg\",            # Use AvgPool instead of MaxPool\n",
    "    global_pool=\"max\"           # Use AdaptiveMaxPool for final pooling\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dce9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "\n",
    "from torchinfo import summary\n",
    "model_summary = summary(model_flexible, input_size=images.shape, col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "\n",
    "# Alternative model summary\n",
    "#from torchsummary import summary\n",
    "#summary(model_flexible, input_size=input_size_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for - train model\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_flexible.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "# Train with early stopping + scheduler\n",
    "history = train_model(\n",
    "    model=model_flexible,\n",
    "    num_epochs=epochs,\n",
    "    train_dl=train_dataloader,\n",
    "    valid_dl=validation_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device='cpu',\n",
    "    verbose=True,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test dataset\n",
    "model_flexible.eval()\n",
    "device = 'cpu'\n",
    "\n",
    "f1_metric = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "f1_score_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        preds = model_flexible(x_batch)  # shape: [batch_size, num_classes]\n",
    "        correct += (preds.argmax(dim=1) == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        # Update F1 metric\n",
    "        f1_score_total += f1_metric(preds, y_batch).item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "avg_f1_score = f1_score_total / len(test_dataloader)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Average F1-score: {avg_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model and summary to file\n",
    "if SAVE_MODELS:\n",
    "    path_summary = Path(models_path / f'{timestamp}_model_summary_number_images{images_for_train_validate_test}_test_accuracy{test_accuracy:.4f}_avgf1_{avg_f1_score:.4f}.txt')\n",
    "    with open(path_summary, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(model_summary))\n",
    "    \n",
    "    path_model = Path(models_path / f'{timestamp}_model_summary_number_images{images_for_train_validate_test}_test_accuracy{test_accuracy:.4f}_avgf1_{avg_f1_score:.4f}_weights.pth')\n",
    "    torch.save(model_flexible.state_dict(), path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss and accuracy\n",
    "\n",
    "plot_training_history(history, images_for_train_validate_test, path=figures_path, save_figures=SAVE_FIGURES, show_plot=SHOW_PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions and true labels\n",
    "\n",
    "n_samples = 12\n",
    "if len(test_dataset) > n_samples:\n",
    "    # Auto layout (2x6 for 12 samples)\n",
    "    plot_random_predictions(model_flexible, test_dataset, num_samples=n_samples, path=figures_path, time= timestamp, number_images=images_for_train_validate_test, rows=2, cols=6, device=\"cpu\", save_figures=SAVE_FIGURES, show_plot=SHOW_PLOT)\n",
    "\n",
    "n_samples = 4\n",
    "if len(test_dataset) > n_samples:\n",
    "    # Custom layout (2x2 for 4 samples)\n",
    "    plot_random_predictions(model_flexible, test_dataset, num_samples=n_samples, path=figures_path, time= timestamp, number_images=images_for_train_validate_test, rows=2, cols=2, device=\"cpu\", save_figures=SAVE_FIGURES, show_plot=SHOW_PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filter weights (first convolutional layer)\n",
    "\n",
    "plot_filter_weights(model_flexible, figures_path, timestamp, images_for_train_validate_test, rows=2, cols=2, channel=0, save_figures=SAVE_FIGURES, show_plot=SHOW_PLOT) # channel as in RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature maaps for a sample image\n",
    "\n",
    "\n",
    "# Random sample from test dataset\n",
    "random_index = np.random.randint(0, len(test_dataset))\n",
    "sample_img, _ = test_dataset[random_index]\n",
    "\n",
    "# Plot sample image\n",
    "plot_image(sample_img, figures_path, timestamp, images_for_train_validate_test, title='Image sample from test dataset', save_figures=SAVE_FIGURES, show_plot=SHOW_PLOT)\n",
    "\n",
    "# Plot all Conv2D\n",
    "for i, layer in enumerate(model_flexible.features):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        plot_feature_maps(model_flexible, i,  sample_img, figures_path, timestamp, images_for_train_validate_test, layers_to_show=[0,3], num_maps=4, rows=2, cols=2, cmap='gray', save_figures=SAVE_FIGURES, show_plot=SHOW_PLOT)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4155 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
