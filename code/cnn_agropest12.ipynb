{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc947afe",
   "metadata": {},
   "source": [
    "# Notebook for classification with CNN and Pytorch of the AgroPest-12 dataset\n",
    "\n",
    "Functions for project specific code can be found in the source folder.\n",
    "\n",
    "Python files starting with cnn* and optuna.py are relevant for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- standard and third party library\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    "    MulticlassConfusionMatrix\n",
    ")\n",
    "\n",
    "# --- project-specific imports\n",
    "from source.cnn_model import train_model, FlexibleCNN, get_batch_size\n",
    "from source.cnn_plotting import (\n",
    "    plot_random_image_per_class,\n",
    "    plot_random_predictions,\n",
    "    plot_filter_weights,\n",
    "    plot_image,\n",
    "    plot_feature_maps,\n",
    "    plot_loss_accuracy,\n",
    "    find_conv_layers\n",
    ")\n",
    "\n",
    "from source.cnn_quality_control import label_histogram, evaluate_classification\n",
    "from source.cnn_retrieve_images import set_seed, classification_collate_fn, YOLODataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9f33d",
   "metadata": {},
   "source": [
    "## Classification with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c93de1",
   "metadata": {},
   "source": [
    "### Settings and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining random seed \n",
    "seed_number = 42\n",
    "set_seed(seed_number, deterministic=True)\n",
    "gen = torch.Generator().manual_seed(seed_number) # for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d673b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Documentation for which classes to use in next section\n",
    "Available classes in Agropest-12\n",
    "  [\n",
    "    \"Ants\",\n",
    "    \"Bees\",\n",
    "    \"Beetles\",\n",
    "    \"Caterpillars\",\n",
    "    \"Earthworms\",\n",
    "    \"Earwigs\",\n",
    "    \"Grasshoppers\",\n",
    "    \"Moths\",\n",
    "    \"Slugs\",\n",
    "    \"Snails\",\n",
    "    \"Wasps\",\n",
    "    \"Weevils\",\n",
    "  ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "yaml_path = Path.cwd().parent / \"datasets\" / \"agropest12\" / \"data.yaml\"\n",
    "\n",
    "# Settings for retreiving images            . Set None for all images or an integer for subset for testing code\n",
    "subset_classes = None # None for all classes, or a list of classes. Example ['Ants', 'Snails'], can be used for testing code\n",
    "n_images_train  = None #None for all images. Choose a small number for testing code\n",
    "n_images_valid =  None # None for all images, or for example 20% of all images int(n_images_train * 0.2). Latter can be applicable for testing code\n",
    "n_images_test = None # None for all images, or for example 20% of all images int(n_images_train * 0.2). Latter can be applicable for testing code\n",
    "\n",
    "batch_size = get_batch_size(n_images_train) \n",
    "\n",
    "\n",
    "PRETRAINED_MODEL = False   # True - use pretrained model and weights, else custom or Optuna hyperparamter search\n",
    "if PRETRAINED_MODEL:\n",
    "    NORMALIZE = 'ImageNet' #'auto' -  load if file for normalized values exists, else compute and save. 'compute' - always compute normalization and save/overwrite file. 'load' - load from file. 'ImageNet' - use ImageNet values\n",
    "else:\n",
    "    NORMALIZE = 'load'\n",
    "NORMALIZE_CACHE_PATH =  Path.cwd().parent / \"datasets\" / \"agropest12\" / \"cache\" / \"agropest12_norm.json\"\n",
    "\n",
    "IMG_SIZE= (224,224) # Pretrained must use (224,224) to match ImageNet. (128, 128) used when using Optuna hyperparamter search.\n",
    "CROP_MODE = 'largest'# 'largest' #'largest'  # 'none' or 'largest  # 'largest' extracts part of image where largest object is located for further processing. Some images have mostly background and small portion of object\n",
    "\n",
    "DEBUG = False # Print debug info\n",
    "QUALITY_CONTROL = True # process, print and plot information for quality control\n",
    "COMPUTE_AVERAGE_IMG_SIZE = True # used in preprocessing and quality control to determine size of cropped images to object.\n",
    "\n",
    "SAVE_MODELS = True\n",
    "SHOW_PLOT = True\n",
    "SAVE_FIGURE = True\n",
    "\n",
    "OPTUNA = False # to run Optuna hyperparameter search or not\n",
    "OPTUNA_TRAILS = 50 # Number of trail for different hyperparameteres. Choose a small number for testing code \n",
    "OPTUNA_EPOCHS = 30 # epochs to run CNN when using Optuna. Choose a small number for testing code (Optuna trails, epochs and n_images_train must be large enough get variance in Optuna results, for example (10, 10 and 20))\n",
    " \n",
    "N_EPOCHS = 50 # epochs to run CNN (not using Optuna). Choose a small number for testing code\n",
    "\n",
    "\n",
    "# Paths for saving figures and models\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "\n",
    "# Directory for saving figures\n",
    "figures_path = parent_dir / \"figures\"\n",
    "figures_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Directory for saving models and summaries\n",
    "models_path = parent_dir / \"models\"\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Current timestamp for files and figures\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes a normalization for agropest dataset, leave to False to retreive normalization values from disk\n",
    "# Computation heavy, only use one time and read normalization from disk later.\n",
    "\n",
    "COMPUTE_NORMALIZATION = False \n",
    "\n",
    "if COMPUTE_NORMALIZATION:\n",
    "    dataset_train_norm = YOLODataset(\n",
    "        yaml_path=yaml_path,\n",
    "        subset_classes=None,\n",
    "        max_images=None, \n",
    "        split='train',\n",
    "        img_size=IMG_SIZE,\n",
    "        normalize='compute',\n",
    "        norm_file=NORMALIZE_CACHE_PATH,\n",
    "        crop_strategy='none',\n",
    "        debug=True,\n",
    "        auto_norm_sample=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset dependent on settings\n",
    "\n",
    "workers = 2 # number of subprocesses\n",
    "\n",
    "# Train\n",
    "dataset_train = YOLODataset(\n",
    "    yaml_path=yaml_path,\n",
    "    subset_classes=subset_classes,\n",
    "    max_images=n_images_train,\n",
    "    split='train',\n",
    "    img_size=IMG_SIZE,\n",
    "    normalize=NORMALIZE,\n",
    "    norm_file=NORMALIZE_CACHE_PATH,\n",
    "    crop_strategy=CROP_MODE,\n",
    "    debug=True,\n",
    ")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=classification_collate_fn, generator=gen, num_workers=workers)\n",
    "\n",
    "# Valid\n",
    "dataset_valid = YOLODataset(\n",
    "    yaml_path=yaml_path,\n",
    "    subset_classes=subset_classes,\n",
    "    max_images=n_images_valid,\n",
    "    split='valid',\n",
    "    img_size=IMG_SIZE,\n",
    "    normalize=NORMALIZE,\n",
    "    norm_file=NORMALIZE_CACHE_PATH,\n",
    "    crop_strategy=CROP_MODE,\n",
    "    debug=DEBUG,\n",
    ")\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True, collate_fn=classification_collate_fn, generator=gen, num_workers=workers)\n",
    "\n",
    "# Test\n",
    "dataset_test = YOLODataset(\n",
    "    yaml_path=yaml_path,\n",
    "    subset_classes=subset_classes,\n",
    "    max_images=n_images_test,\n",
    "    split='test',\n",
    "    img_size=IMG_SIZE,\n",
    "    normalize=NORMALIZE,\n",
    "    norm_file=NORMALIZE_CACHE_PATH,\n",
    "    crop_strategy=CROP_MODE,\n",
    "    debug=DEBUG,\n",
    ")\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=classification_collate_fn, generator=gen, num_workers=workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4f17d",
   "metadata": {},
   "source": [
    "#### Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24963c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average size for cropped image. Information to use for Transform and resize image\n",
    "\n",
    "if COMPUTE_AVERAGE_IMG_SIZE:\n",
    "    from statistics import mean\n",
    "\n",
    "    N = len(dataset_train)\n",
    "    crop_sizes = []\n",
    "    final_sizes = []\n",
    "\n",
    "    for i in range(N):\n",
    "        s = dataset_train[i]\n",
    "        crop_sizes.append(s[\"crop_size\"])  # (W,H) before resize\n",
    "        final_sizes.append((s[\"image\"].shape[2], s[\"image\"].shape[1]))  # (W,H) after transform\n",
    "\n",
    "    # Compute averages\n",
    "    avg_crop_w = mean([w for w, h in crop_sizes])\n",
    "    avg_crop_h = mean([h for w, h in crop_sizes])\n",
    "\n",
    "    print(f\"Average cropped size: (Width: {avg_crop_w:.1f}, Height: {avg_crop_h:.1f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ce7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about images in batch as a quality control to see that import of images are OK\n",
    "if QUALITY_CONTROL:\n",
    "    from source.cnn_quality_control import validate_dataset\n",
    "    validate_dataset(dataset_test, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b26620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one image from each class in subset\n",
    "if QUALITY_CONTROL: plot_random_image_per_class(dataset_train, verbose=True, save_figure=SAVE_FIGURE, show_plot=SHOW_PLOT, figures_path=figures_path, timestamp=timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a661eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number images in each class for train, validation and train daset\n",
    "if QUALITY_CONTROL: \n",
    "    print()\n",
    "    print('Training dataset')\n",
    "    _ = label_histogram(dataset_train)\n",
    "\n",
    "    print()\n",
    "    print('Validation dataset')\n",
    "    _ = label_histogram(dataset_valid)\n",
    "\n",
    "    print()\n",
    "    print('Test dataset')\n",
    "    _ = label_histogram(dataset_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2db54",
   "metadata": {},
   "source": [
    "## Generate and train PyTorch CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f984c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of class  to 12 if subset_classes is set to None\n",
    "if subset_classes == None:\n",
    "    n_classes = 12\n",
    "else:\n",
    "    n_classes = len(subset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights from Resnet\n",
    "\n",
    "if PRETRAINED_MODEL:\n",
    "    import torchvision\n",
    "    from torchvision.models import ResNet18_Weights\n",
    "    weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "    model_pretrained = torchvision.models.resnet18(weights=weights)\n",
    "\n",
    "\n",
    "    # Agropest 12 is a small dataset --> Freeze all layers\n",
    "    for param in model_pretrained.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    num_classes = n_classes  \n",
    "    # train only fully connected layer for AgroPest-12 and 12 classes\n",
    "    model_pretrained.fc = nn.Linear(model_pretrained.fc.in_features, num_classes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive number of channels in image and update input size\n",
    "batch = next(iter(dataloader_test))\n",
    "images = batch['images']\n",
    "channels = images.shape[1]\n",
    "input_dim = (channels,) + IMG_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdb01c",
   "metadata": {},
   "source": [
    "### Optuna option to tune hyperparameters (not used with pretrained model and weights in this project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ce14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna hyperparameter search\n",
    "if OPTUNA and not PRETRAINED_MODEL:\n",
    "    from source.optuna import create_objective\n",
    "    import optuna\n",
    "    objective = create_objective(n_classes, input_dim, dataset_train, dataset_valid, num_epochs=OPTUNA_EPOCHS, device='cpu')\n",
    "\n",
    "    db_path = models_path / f\"{timestamp}_optuna_study.db\"\n",
    "    optuna_path = f\"sqlite:///{db_path}\"  \n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=1, n_min_trials=5),\n",
    "        storage=optuna_path,\n",
    "        load_if_exists=False\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=OPTUNA_TRAILS, timeout=None, gc_after_trial=True)\n",
    "\n",
    "    print(\"Best value (valid_acc):\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae782f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTUNA and not PRETRAINED_MODEL:\n",
    "    from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "    plot_optimization_history(study).show()\n",
    "    plot_param_importances(study).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85db142",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTUNA and not PRETRAINED_MODEL:\n",
    "    print('Hei')\n",
    "    # Extract best parameters from Optuna\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Conv layers based on n_conv_layers and filters\n",
    "    n_conv_layers = best_params['n_conv_layers']\n",
    "    conv_layers = [\n",
    "        (best_params[f'conv{i+1}_filters'], 3)  # fixed kernel size\n",
    "        for i in range(n_conv_layers)\n",
    "    ]\n",
    "\n",
    "    # FC layers: optional hidden + fixed output\n",
    "    fc_layers = [n_classes]\n",
    "    if best_params['use_hidden_layer']:\n",
    "        fc_layers.insert(0, best_params['fc_hidden_neurons'])\n",
    "\n",
    "    # Dropout values\n",
    "    dropout_fc = best_params['dropout_fc']\n",
    "    dropout_conv = best_params['dropout_conv']\n",
    "\n",
    "    activation_choice = nn.ReLU  # fixed activation\n",
    "    use_batchnorm = True  # always enabled\n",
    "\n",
    "else:\n",
    "    # Fallback to predefined custom model architecture\n",
    "    conv_layers = [(32, 3), (64, 3), (128, 3), (256, 3)]\n",
    "    fc_layers = [n_classes]\n",
    "    activation_choice = nn.ReLU\n",
    "    dropout_fc = 0.3\n",
    "    dropout_conv = 0.0\n",
    "    use_batchnorm = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385135fa",
   "metadata": {},
   "source": [
    "### Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "model_custom = FlexibleCNN(\n",
    "    input_size=input_dim,\n",
    "    num_classes=n_classes,\n",
    "    conv_layers=conv_layers,\n",
    "    fc_layers=fc_layers,\n",
    "    activation=activation_choice,\n",
    "    dropout_fc=dropout_fc,\n",
    "    dropout_conv=dropout_conv,\n",
    "    use_batchnorm=use_batchnorm,\n",
    "    pool_type=\"max\",\n",
    "    global_pool=\"avg\"\n",
    ").to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47704d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model in use dependent on pretrained or custom/optuna\n",
    "\n",
    "if PRETRAINED_MODEL:\n",
    "    model = model_pretrained\n",
    "    print('Pretrained model')\n",
    "else:\n",
    "    model = model_custom\n",
    "    print('Custom model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save model summary for PyTorch model and parameters\n",
    "path_summary = Path(models_path / f'{timestamp}_model_summary_pretrained{PRETRAINED_MODEL}.txt')\n",
    "\n",
    "if SAVE_MODELS:\n",
    "    with open(path_summary, \"w\") as f:\n",
    "        sys.stdout = f\n",
    "        summary(model, input_dim, batch_size=batch_size)\n",
    "        sys.stdout = sys.__stdout__  \n",
    "\n",
    "\n",
    "# Alternative model summary\n",
    "#from torchinfo import summary\n",
    "#summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db489f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# training based on Optuna or not\n",
    "learning_rate = best_params['learning_rate'] if OPTUNA else 0.001\n",
    "\n",
    "if PRETRAINED_MODEL:\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=1e-4)  # only updates weights for fully connected layer\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = N_EPOCHS \n",
    "\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    num_epochs=epochs,\n",
    "    train_dl=dataloader_train,\n",
    "    valid_dl=dataloader_valid,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device='cpu',\n",
    "    verbose=True,\n",
    "    patience_val=15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce182db",
   "metadata": {},
   "source": [
    "## Results from classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss and accuracy by epoch\n",
    "plot_loss_accuracy(history, figures_path, timestamp, save_figure=SAVE_FIGURE, show_plot=SHOW_PLOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059363a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluvate classification, print metric and plot confusion matrix\n",
    "\n",
    "results = evaluate_classification(\n",
    "    model=model,\n",
    "    dataloader=dataloader_test,\n",
    "    num_classes=n_classes,\n",
    "    figures_path=figures_path,\n",
    "    timestamp=timestamp,\n",
    "    device='cpu',\n",
    "    class_names=subset_classes,\n",
    "    save_figure=SAVE_FIGURE,\n",
    "    show_plot=SHOW_PLOT\n",
    ")\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model and summary to file\n",
    "if SAVE_MODELS:\n",
    "    path_model = Path(models_path / f'{timestamp}_model_summary_test-img_size{IMG_SIZE}_accuracy{results['accuracy']:.4f}_avgf1_{results['macro_f1']:.4f}_weights.pth')\n",
    "    torch.save(model.state_dict(), path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3eecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures of random predictions\n",
    "\n",
    "n_classes = len(dataset_test.subset_class_names)  # e.g., 2 for ['Ants', 'Snails']\n",
    "n_cols = min(2, n_classes)\n",
    "n_rows = math.ceil(n_classes / n_cols)\n",
    "\n",
    "plot_random_predictions(\n",
    "    model,\n",
    "    dataset_test,\n",
    "    num_samples=n_classes,           \n",
    "    path=figures_path,\n",
    "    time=timestamp,\n",
    "    number_images=n_images_train,\n",
    "    rows=n_rows, cols=n_cols,\n",
    "    class_names=dataset_test.subset_class_names,\n",
    "    device=\"cpu\",\n",
    "    save_figures=SAVE_FIGURE,\n",
    "    show_plot=SHOW_PLOT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot filter weights\n",
    "plot_filter_weights(model, figures_path, timestamp, n_images_train, rows=2, cols=2, channel=0, save_figures=SAVE_FIGURE, show_plot=SHOW_PLOT) # channel as in RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature maps for a sample image\n",
    "\n",
    "# Random sample from test dataset\n",
    "random_index = np.random.randint(0, len(dataset_test))\n",
    "sample = dataset_test[random_index]\n",
    "sample_img = sample[\"image\"]\n",
    "true_name = sample.get(\"class_name\", \"\")\n",
    "\n",
    "plot_image(\n",
    "    sample_img,\n",
    "    figures_path,\n",
    "    timestamp,\n",
    "    n_images_train,\n",
    "    title=f'Image sample from test dataset (True: {true_name})',\n",
    "    save_figures=SAVE_FIGURE,\n",
    "    show_plot=SHOW_PLOT,\n",
    "    dataset=dataset_test  \n",
    ")\n",
    "\n",
    "# Retreive convolutional layers\n",
    "conv_layers = find_conv_layers(model)\n",
    "indices_to_show = [0, len(conv_layers) - 1]  # first and last convolutional layer\n",
    "for idx, (name, layer) in enumerate(conv_layers):\n",
    "    if idx in indices_to_show: \n",
    "        plot_feature_maps(\n",
    "            model=model,\n",
    "            image=sample_img,            \n",
    "            path=figures_path,\n",
    "            time=timestamp,\n",
    "            number_images=n_images_train,\n",
    "            layers_to_show=[idx],              \n",
    "            num_maps=4,\n",
    "            rows=2, cols=2,\n",
    "            cmap=\"gray\",\n",
    "            save_figures=SAVE_FIGURE,\n",
    "            show_plot=SHOW_PLOT,\n",
    "            device=\"cpu\",\n",
    "            input_is_tensor=True,        \n",
    "            preprocess=None\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4155 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
